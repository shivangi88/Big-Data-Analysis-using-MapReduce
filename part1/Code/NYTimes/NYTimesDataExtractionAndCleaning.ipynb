{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nytimesarticle import articleAPI\n",
    "import numpy as np\n",
    "from requests import get        \n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from time import sleep\n",
    "\n",
    "def parse_articles(articles):\n",
    "    '''\n",
    "    This function takes in a response to the NYT api and parses\n",
    "    the articles into a list of dictionaries\n",
    "    '''\n",
    "    news = []\n",
    "    for i in articles['response']['docs']:\n",
    "        dic = {}\n",
    "        dic['url'] = i['web_url']\n",
    "        news.append(dic)\n",
    "    return(news)\n",
    "\n",
    "api = articleAPI('Article API Key')\n",
    "i=0\n",
    "all_url_list=[]\n",
    "all_articles1=[]\n",
    "\n",
    "# politics -> elections, trump, campaign, obama, sanders, healthcare\n",
    "\n",
    "while(i<10):\n",
    "    articles= api.search(q = \"republican\",page=i,begin_date = 20190406)\n",
    "    articles1 = parse_articles(articles)\n",
    "    all_articles1 = all_articles1 + articles1\n",
    "    i=i+1\n",
    "\n",
    "#print(\"Articles:-\", all_articles,sep=\"\\n\")\n",
    "url_list=[i[\"url\"] for i in all_articles1]\n",
    "with open('nytimes_republican.txt', 'w') as f:\n",
    "    for item in url_list:\n",
    "        f.write(\"%s\\n\" % item) \n",
    "        \n",
    "        \n",
    "# web scraping to extract text from website using beautiful soup\n",
    "\n",
    "#word_tokenize accepts a string as an input, not a file. \n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"words\")\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words \n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words=set(words.words())\n",
    "\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "####################### ELECTION ###########################\n",
    "\n",
    "list_2 = open('nytimes_election.txt', 'r')\n",
    "ele_list = list_2.read().splitlines()\n",
    "list_2.close()\n",
    "\n",
    "election_data = []\n",
    "\n",
    "for ele_element in ele_list:\n",
    "    ele_url = ele_element\n",
    "    ele_resp = get(ele_url)\n",
    "    sleep(5)\n",
    "    ele_html_soup = BeautifulSoup(ele_resp.text, 'html.parser')\n",
    "    ele_content = ele_html_soup.findAll(['title', 'p'])\n",
    "    ele_content_text = [a.get_text() for a in ele_content]\n",
    "    election_data.append(ele_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for j in election_data:\n",
    "    app_file = open(\"election_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(j))\n",
    "    app_file.close()\n",
    "\n",
    "### stopping and stemming for election data ###\n",
    "\n",
    "election_read_data = open(\"election_data.txt\",'r').read()\n",
    "election_read_data = election_read_data.replace(\"_\",\" \")\n",
    "\n",
    "election_data_cleaned = re.sub(r'[^\\w]', ' ', election_read_data)\n",
    "election_data_cleaned_1=re.sub(r'[\\d]*','',election_data_cleaned) # remove numbers\n",
    "election_words=word_tokenize(election_data_cleaned_1)\n",
    "\n",
    "for r in election_words:\n",
    "    r = r.lower()\n",
    "    if not r in stop_words:\n",
    "        if r in english_words:\n",
    "            e_wrd=lemmatizer.lemmatize(r)\n",
    "            if(e_wrd.isalpha()):\n",
    "                appendFile = open('election_filtered_text.txt','a')\n",
    "#        print(r + \":\" + e_wrd)\n",
    "                appendFile.write(\" \"+e_wrd) \n",
    "                appendFile.close()        \n",
    "        \n",
    "#############################################################\n",
    "\n",
    "####################### TRUMP ############################\n",
    "\n",
    "list_3 = open('nytimes_trump.txt', 'r')\n",
    "t_list = list_3.read().splitlines()\n",
    "list_3.close()\n",
    "\n",
    "trump_data = []\n",
    "\n",
    "for t_element in t_list:\n",
    "    t_url = t_element\n",
    "    t_resp = get(t_url)\n",
    "    sleep(5)\n",
    "    t_html_soup = BeautifulSoup(t_resp.text, 'html.parser')\n",
    "    t_content = t_html_soup.findAll(['title', 'p'])\n",
    "    t_content_text = [a.get_text() for a in t_content]\n",
    "    trump_data.append(t_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for k in trump_data:\n",
    "    app_file = open(\"trump_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(k))\n",
    "    app_file.close()\n",
    "\n",
    "trump_read_data = open(\"trump_data.txt\",'r').read()\n",
    "trump_read_data = trump_read_data.replace(\"_\",\" \")\n",
    "\n",
    "trump_data_cleaned = re.sub(r'[^\\w]', ' ', trump_read_data)\n",
    "trump_data_cleaned_1=re.sub(r'[\\d]*','',trump_data_cleaned) # remove numbers\n",
    "trump_words=word_tokenize(trump_data_cleaned_1)\n",
    "\n",
    "### stopping and stemmming for trump data ###\n",
    "\n",
    "for s in trump_words: \n",
    "    s = s.lower()\n",
    "    if not s in stop_words:\n",
    "        if s in english_words:        \n",
    "            t_wrd=lemmatizer.lemmatize(s)\n",
    "            if(t_wrd.isalpha()):\n",
    "                appendFile = open('trump_filtered_text.txt','a')\n",
    "#        print(s + \":\" + t_wrd)\n",
    "                appendFile.write(\" \"+t_wrd) \n",
    "                appendFile.close()        \n",
    "        \n",
    "##############################################################\n",
    "\n",
    "########################## CAMPAIGN ##########################\n",
    "\n",
    "list_4 = open('nytimes_campaign.txt', 'r')\n",
    "c_list = list_4.read().splitlines()\n",
    "list_4.close()\n",
    "\n",
    "campaign_data = []\n",
    "\n",
    "for c_element in c_list:\n",
    "    c_url = c_element\n",
    "    c_resp = get(c_url)\n",
    "    sleep(5)\n",
    "    c_html_soup = BeautifulSoup(c_resp.text, 'html.parser')\n",
    "    c_content = c_html_soup.findAll(['title', 'p'])\n",
    "    c_content_text = [a.get_text() for a in c_content]\n",
    "    campaign_data.append(c_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for l in campaign_data:\n",
    "    app_file = open(\"campaign_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(l))\n",
    "    app_file.close()\n",
    "\n",
    "### stopping and stemmming for campaign data ###\n",
    "\n",
    "campaign_read_data = open(\"campaign_data.txt\",'r').read()\n",
    "campaign_read_data = campaign_read_data.replace(\"_\",\" \")\n",
    "\n",
    "campaign_data_cleaned = re.sub(r'[^\\w]', ' ', campaign_read_data)\n",
    "campaign_data_cleaned_1=re.sub(r'[\\d]*','',campaign_data_cleaned) # remove numbers\n",
    "campaign_words=word_tokenize(campaign_data_cleaned_1)\n",
    "\n",
    "for t in campaign_words: \n",
    "    t = t.lower()\n",
    "    if not t in stop_words:\n",
    "        if t in english_words:                \n",
    "            c_wrd=lemmatizer.lemmatize(t)\n",
    "            if(c_wrd.isalpha()):            \n",
    "                appendFile = open('campaign_filtered_text.txt','a')\n",
    "        #        print(t + \":\" + c_wrd)\n",
    "                appendFile.write(\" \"+c_wrd) \n",
    "                appendFile.close()        \n",
    "\n",
    "################################################################\n",
    "\n",
    "########################## IMMIGRATION ##########################\n",
    "\n",
    "list_5 = open('nytimes_immigration.txt', 'r')\n",
    "i_list = list_5.read().splitlines()\n",
    "list_5.close()\n",
    "\n",
    "immigration_data = []\n",
    "\n",
    "for i_element in i_list:\n",
    "    i_url = i_element\n",
    "    i_resp = get(i_url)\n",
    "    sleep(5)\n",
    "    i_html_soup = BeautifulSoup(i_resp.text, 'html.parser')\n",
    "    i_content = i_html_soup.findAll(['title', 'p'])\n",
    "    i_content_text = [a.get_text() for a in i_content]\n",
    "    immigration_data.append(i_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for m in immigration_data:\n",
    "    app_file = open(\"immigration_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(m))\n",
    "    app_file.close()\n",
    "\n",
    "### stopping and stemmming for immigration data ###\n",
    "\n",
    "immigration_read_data = open(\"immigration_data.txt\",'r').read()\n",
    "immigration_read_data = immigration_read_data.replace(\"_\",\" \")\n",
    "\n",
    "immigration_data_cleaned = re.sub(r'[^\\w]', ' ', immigration_read_data)\n",
    "immigration_data_cleaned_1=re.sub(r'[\\d]*','',immigration_data_cleaned) # remove numbers\n",
    "immigration_words=word_tokenize(immigration_data_cleaned_1)\n",
    "\n",
    "for u in immigration_words: \n",
    "    u = u.lower()\n",
    "    if not u in stop_words:\n",
    "        if u in english_words:                        \n",
    "            i_wrd=lemmatizer.lemmatize(u)\n",
    "            if(i_wrd.isalpha()):                        \n",
    "                appendFile = open('immigration_filtered_text.txt','a')\n",
    "        #        print(u + \":\" + i_wrd)\n",
    "                appendFile.write(\" \"+i_wrd) \n",
    "                appendFile.close()        \n",
    "\n",
    "################################################################\n",
    "\n",
    "########################## ECONOMY #############################\n",
    "\n",
    "list_6 = open('nytimes_economy.txt', 'r')\n",
    "eco_list = list_6.read().splitlines()\n",
    "list_6.close()\n",
    "\n",
    "economy_data = []\n",
    "\n",
    "for eco_element in eco_list:\n",
    "    eco_url = eco_element\n",
    "    eco_resp = get(eco_url)\n",
    "    sleep(5)\n",
    "    eco_html_soup = BeautifulSoup(eco_resp.text, 'html.parser')\n",
    "    eco_content = eco_html_soup.findAll(['title', 'p'])\n",
    "    eco_content_text = [a.get_text() for a in eco_content]\n",
    "    economy_data.append(eco_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for n in economy_data:\n",
    "    app_file = open(\"economy_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(n))\n",
    "    app_file.close()\n",
    "\n",
    "### stopping and stemmming for economy data ###\n",
    "\n",
    "economy_read_data = open(\"economy_data.txt\",'r').read()\n",
    "economy_read_data = economy_read_data.replace(\"_\",\" \")\n",
    "\n",
    "economy_data_cleaned = re.sub(r'[^\\w]', ' ', economy_read_data) # remove symbols\n",
    "economy_data_cleaned_1=re.sub(r'[\\d]*','',economy_data_cleaned) # remove numbers\n",
    "economy_words=word_tokenize(economy_data_cleaned_1)\n",
    "\n",
    "for v in economy_words: \n",
    "    v = v.lower()\n",
    "    if not v in stop_words:\n",
    "        if v in english_words:                        \n",
    "            eco_wrd=lemmatizer.lemmatize(v)\n",
    "            if(eco_wrd.isalpha()):                                    \n",
    "                appendFile = open('economy_filtered_text.txt','a')\n",
    "        #        print(v + \":\" + eco_wrd)\n",
    "                appendFile.write(\" \"+eco_wrd) \n",
    "                appendFile.close()        \n",
    "\n",
    "################################################################\n",
    "        \n",
    "########################## Healthcare #############################\n",
    "\n",
    "list_7 = open('nytimes_healthcare.txt', 'r')\n",
    "h_list = list_7.read().splitlines()\n",
    "list_7.close()\n",
    "\n",
    "healthcare_data = []\n",
    "\n",
    "for h_element in h_list:\n",
    "    h_url = h_element\n",
    "    h_resp = get(h_url)\n",
    "    sleep(5)\n",
    "    h_html_soup = BeautifulSoup(h_resp.text, 'html.parser')\n",
    "    h_content = h_html_soup.findAll(['title', 'p'])\n",
    "    h_content_text = [a.get_text() for a in h_content]\n",
    "    healthcare_data.append(h_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for c in healthcare_data:\n",
    "    app_file = open(\"healthcare_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(c))\n",
    "    app_file.close()\n",
    "\n",
    "### stopping and stemmming for economy data ###\n",
    "\n",
    "healthcare_read_data = open(\"healthcare_data.txt\",'r').read()\n",
    "healthcare_read_data = healthcare_read_data.replace(\"_\",\" \")\n",
    "\n",
    "healthcare_data_cleaned = re.sub(r'[^\\w]', ' ', healthcare_read_data) # remove symbols\n",
    "healthcare_data_cleaned_1=re.sub(r'[\\d]*','',healthcare_data_cleaned) # remove numbers\n",
    "healthcare_words=word_tokenize(healthcare_data_cleaned_1)\n",
    "\n",
    "for w in healthcare_words:\n",
    "    w = w.lower()\n",
    "    if not w in stop_words:\n",
    "        if w in english_words:                                \n",
    "            h_wrd=lemmatizer.lemmatize(w)\n",
    "            if(h_wrd.isalpha()):                                                \n",
    "                appendFile = open('healthcare_filtered_text.txt','a')\n",
    "        #        print(w + \":\" + h_wrd)\n",
    "                appendFile.write(\" \"+h_wrd) \n",
    "                appendFile.close()        \n",
    "\n",
    "################################################################        \n",
    "        \n",
    "########################## Republican #############################\n",
    "\n",
    "list_8 = open('nytimes_republican.txt', 'r')\n",
    "r_list = list_8.read().splitlines()\n",
    "list_8.close()\n",
    "\n",
    "republican_data = []\n",
    "\n",
    "for r_element in r_list:\n",
    "    r_url = r_element\n",
    "    r_resp = get(r_url)\n",
    "    sleep(5)\n",
    "    r_html_soup = BeautifulSoup(r_resp.text, 'html.parser')\n",
    "    r_content = r_html_soup.findAll(['title', 'p'])\n",
    "    r_content_text = [a.get_text() for a in r_content]\n",
    "    republican_data.append(r_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for p in republican_data:\n",
    "    app_file = open(\"republican_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(p))\n",
    "    app_file.close()\n",
    "\n",
    "### stopping and stemmming for economy data ###\n",
    "\n",
    "republican_read_data = open(\"republican_data.txt\",'r').read()\n",
    "republican_read_data = republican_read_data.replace(\"_\",\" \")\n",
    "\n",
    "republican_data_cleaned = re.sub(r'[^\\w]', ' ', republican_read_data) # remove symbols\n",
    "republican_data_cleaned_1=re.sub(r'[\\d]*','', republican_data_cleaned) # remove numbers\n",
    "republican_words=word_tokenize(republican_data_cleaned_1)\n",
    "\n",
    "for x in republican_words:\n",
    "    x = x.lower()\n",
    "    if not x in stop_words:\n",
    "        if x in english_words:                                        \n",
    "            r_wrd=lemmatizer.lemmatize(x)\n",
    "            if(r_wrd.isalpha()):                                                            \n",
    "                appendFile = open('republican_filtered_text.txt','a')\n",
    "        #        print(x + \":\" + r_wrd)\n",
    "                appendFile.write(\" \"+r_wrd) \n",
    "                appendFile.close()        \n",
    "\n",
    "################################################################        \n",
    "        \n",
    "# Appending in 1 file\n",
    "        \n",
    "txt_names = ['election_filtered_text.txt','trump_filtered_text.txt',\n",
    "             'campaign_filtered_text.txt','immigration_filtered_text.txt',\n",
    "             'economy_filtered_text.txt','healthcare_filtered_text.txt',\n",
    "             'republican_filtered_text.txt']        \n",
    "with open('NYTimes_data.txt','w') as outf:\n",
    "    for fname in txt_names:\n",
    "        with open(fname) as infile:\n",
    "            outf.write(infile.read())\n",
    "        \n",
    "        \n",
    "######################### Small data ########################        \n",
    "\n",
    "\n",
    "list_8 = open('nytimes_small_republican.txt', 'r')\n",
    "r_list = list_8.read().splitlines()\n",
    "list_8.close()\n",
    "\n",
    "republican_data = []\n",
    "\n",
    "for r_element in r_list:\n",
    "    r_url = r_element\n",
    "    r_resp = get(r_url)\n",
    "    sleep(5)\n",
    "    r_html_soup = BeautifulSoup(r_resp.text, 'html.parser')\n",
    "    r_content = r_html_soup.findAll(['title', 'p'])\n",
    "    r_content_text = [a.get_text() for a in r_content]\n",
    "    republican_data.append(r_content_text)\n",
    "\n",
    "# looping and creating a text file from the extracted text\n",
    "for p in republican_data:\n",
    "    app_file = open(\"small_data.txt\",'a')\n",
    "    app_file.write(\" \"+ str(p))\n",
    "    app_file.close()\n",
    "\n",
    "### stopping and stemmming for economy data ###\n",
    "\n",
    "republican_read_data = open(\"small_data.txt\",'r').read()\n",
    "republican_read_data = republican_read_data.replace(\"_\",\" \")\n",
    "\n",
    "republican_data_cleaned = re.sub(r'[^\\w]', ' ', republican_read_data) # remove symbols\n",
    "republican_data_cleaned_1=re.sub(r'[\\d]*','', republican_data_cleaned) # remove numbers\n",
    "republican_words=word_tokenize(republican_data_cleaned_1)\n",
    "\n",
    "for x in republican_words:\n",
    "    x = x.lower()\n",
    "    if not x in stop_words:\n",
    "        if x in english_words:                                        \n",
    "            r_wrd=lemmatizer.lemmatize(x)\n",
    "            if(r_wrd.isalpha()):                                                            \n",
    "                appendFile = open('small_filtered_text.txt','a')\n",
    "        #        print(x + \":\" + r_wrd)\n",
    "                appendFile.write(\" \"+r_wrd) \n",
    "                appendFile.close()        \n",
    "        \n",
    "\n",
    "################################################################        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
