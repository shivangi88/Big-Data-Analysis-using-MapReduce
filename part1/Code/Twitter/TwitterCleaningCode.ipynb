{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#word_tokenize accepts a string as an input, not a file. \n",
    "stop_words = set(stopwords.words('english'))\n",
    "english_words=set(words.words())\n",
    "file1 = open(\"total_tweets_small.txt\") \n",
    "line = file1.read()# Use this to read file content as a stream: \n",
    "#word = line.split() \n",
    "line1=re.sub(r'\\[^\\d]+', ' ', line)\n",
    "line2=re.sub(r'[_-_]', ' ', line1)\n",
    "line3=re.sub(r'[^\\w]', ' ', line2)\n",
    "line4=re.sub(r'[\\d]*','',line3)\n",
    "sentence=line4.lower()\n",
    "words=word_tokenize(sentence)\n",
    "wn = WordNetLemmatizer()\n",
    "for r in words: \n",
    "    if not r in stop_words:\n",
    "        if r in english_words:\n",
    "            wrd=wn.lemmatize(r,'v')\n",
    "            if(wrd.isalpha()):\n",
    "                appendFile = open('Twitter_small_data.txt','a') \n",
    "                appendFile.write(\" \"+wrd) \n",
    "                appendFile.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
